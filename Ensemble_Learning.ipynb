{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Ensemble Learning in Machine Learning? Explain the key idea behind it.\n",
        "\n",
        "Answer:\n",
        "Ensemble Learning is a machine learning technique in which multiple models (called base learners) are trained and combined to solve the same problem. Instead of relying on a single model, ensemble methods aggregate the predictions of several models to produce a more accurate and stable result.\n",
        "\n",
        "The key idea behind ensemble learning is that a group of weak or moderately accurate models can work together to form a strong model. Different models may make different errors, and when their predictions are combined (through voting, averaging, or weighted methods), these errors can cancel out.\n",
        "\n",
        "Ensemble learning helps improve:\n",
        "\n",
        "Accuracy\n",
        "\n",
        "Generalization\n",
        "\n",
        "Robustness to noise\n",
        "\n",
        "Resistance to overfitting\n",
        "\n",
        "Popular ensemble techniques include Bagging, Boosting, and Random Forests.\n",
        "\n",
        "Question 2: What is the difference between Bagging and Boosting?\n",
        "\n",
        "Answer:Aspect\tBagging (Bootstrap Aggregating)\tBoosting\n",
        "Training style\tModels are trained independently\tModels are trained sequentially\n",
        "Data sampling\tUses bootstrap sampling (random sampling with replacement)\tFocuses more on misclassified samples\n",
        "Error handling\tTreats all samples equally\tGives higher weight to difficult samples\n",
        "Goal\tReduce variance\tReduce bias and variance\n",
        "Overfitting\tHelps reduce overfitting\tCan overfit if data is noisy\n",
        "Example\tRandom Forest\tAdaBoost, Gradient Boosting\n",
        "\n",
        "Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "\n",
        "Answer:Bootstrap sampling is a technique where multiple datasets are created by randomly sampling from the original dataset with replacement. This means some samples may appear multiple times, while others may not appear at all.\n",
        "\n",
        "In Bagging methods like Random Forest:\n",
        "\n",
        "Each decision tree is trained on a different bootstrap sample\n",
        "\n",
        "This introduces diversity among trees\n",
        "\n",
        "Diverse trees make different errors, reducing overall variance\n",
        "\n",
        "Bootstrap sampling plays a crucial role in making Random Forest models more stable, less prone to overfitting, and better at generalizing.\n",
        "\n",
        "Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
        "\n",
        "Answer:Out-of-Bag (OOB) samples are the data points not selected in a bootstrap sample for training a particular model. On average, about 36% of data is left out in each bootstrap sample.\n",
        "\n",
        "OOB samples are used to:\n",
        "\n",
        "Test each tree on unseen data\n",
        "\n",
        "Estimate model performance without a separate validation set\n",
        "\n",
        "The OOB score is calculated by aggregating predictions on OOB samples and comparing them with true labels.\n",
        "This provides an unbiased estimate of model accuracy, saving computation time and data.\n",
        "\n",
        "Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
        "\n",
        "Answer:Single Decision Tree:\n",
        "\n",
        "Feature importance is based on reduction in impurity\n",
        "\n",
        "Highly sensitive to training data\n",
        "\n",
        "Importance values can be unstable and biased\n",
        "\n",
        "Random Forest:\n",
        "\n",
        "Feature importance is averaged across multiple trees\n",
        "\n",
        "More reliable and stable\n",
        "\n",
        "Reduces bias caused by noisy splits\n",
        "\n",
        "Better reflects true feature relevance"
      ],
      "metadata": {
        "id": "oCgxdhc7x524"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Python Program – Random Forest on Breast Cancer Dataset"
      ],
      "metadata": {
        "id": "DazcNF-zyV_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "importance = pd.Series(rf.feature_importances_, index=feature_names)\n",
        "top_5 = importance.sort_values(ascending=False).head(5)\n",
        "\n",
        "print(top_5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1W2JcBAByYFp",
        "outputId": "58cfdf3d-02cc-4ad8-cd2c-f522fcc35d58"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "worst area              0.139357\n",
            "worst concave points    0.132225\n",
            "mean concave points     0.107046\n",
            "worst radius            0.082848\n",
            "worst perimeter         0.080850\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Python Program – Bagging Classifier vs Decision Tree (Iris Dataset)"
      ],
      "metadata": {
        "id": "ML0yKjvryilA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=1\n",
        ")\n",
        "\n",
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "\n",
        "bag = BaggingClassifier(\n",
        "    n_estimators=50,\n",
        "    random_state=1\n",
        ")\n",
        "bag.fit(X_train, y_train)\n",
        "bag_pred = bag.predict(X_test)\n",
        "\n",
        "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
        "bag_accuracy = accuracy_score(y_test, bag_pred)\n",
        "\n",
        "print(\"Single Decision Tree Accuracy:\", dt_accuracy)\n",
        "print(\"Bagging Classifier Accuracy:\", bag_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TyfSDw20yBg",
        "outputId": "1b206855-ac44-4e92-fa2a-c0910709b1f2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single Decision Tree Accuracy: 0.9555555555555556\n",
            "Bagging Classifier Accuracy: 0.9555555555555556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Python Program – Random Forest with GridSearchCV"
      ],
      "metadata": {
        "id": "svs1NlITy7I_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [None, 5, 10]\n",
        "}\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "grid = GridSearchCV(rf, param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Final Accuracy:\", accuracy_score(y_test, pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaUI8OLUy77V",
        "outputId": "d753c594-a35b-4cfa-9a1f-fdc73f04198a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 5, 'n_estimators': 100}\n",
            "Final Accuracy: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Python Program – Bagging vs Random Forest Regressor"
      ],
      "metadata": {
        "id": "a3TlGdEty_LX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "bag = BaggingRegressor(DecisionTreeRegressor(), n_estimators=50, random_state=42)\n",
        "bag.fit(X_train, y_train)\n",
        "bag_pred = bag.predict(X_test)\n",
        "\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_pred = rf.predict(X_test)\n",
        "\n",
        "print(\"Bagging MSE:\", mean_squared_error(y_test, bag_pred))\n",
        "print(\"Random Forest MSE:\", mean_squared_error(y_test, rf_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7Tgik8zzCyT",
        "outputId": "872f90e0-6a00-421a-fb47-cd4b85176a7d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging MSE: 0.2572988359842641\n",
            "Random Forest MSE: 0.2553684927247781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Real-World Use Case – Loan Default Prediction\n",
        "Answer:\n",
        "\n",
        "Step 1: Choosing Bagging or Boosting\n",
        "If the data is noisy and high-variance, Bagging (Random Forest) is preferred.\n",
        "If misclassification cost is high and patterns are complex, Boosting is useful.\n",
        "\n",
        "Step 2: Handling Overfitting\n",
        "\n",
        "Use ensemble methods\n",
        "\n",
        "Limit tree depth\n",
        "\n",
        "Use cross-validation\n",
        "\n",
        "Regularize models\n",
        "\n",
        "Step 3: Selecting Base Models\n",
        "Decision Trees are chosen due to:\n",
        "\n",
        "Interpretability\n",
        "\n",
        "Ability to handle non-linear data\n",
        "\n",
        "Compatibility with ensembles\n",
        "\n",
        "Step 4: Performance Evaluation\n",
        "\n",
        "Use k-fold cross-validation\n",
        "\n",
        "Metrics: ROC-AUC, Precision, Recall, F1-score\n",
        "\n",
        "Focus on recall to reduce false negatives\n",
        "\n",
        "Step 5: Justification of Ensemble Learning\n",
        "Ensemble learning improves:\n",
        "\n",
        "Prediction accuracy\n",
        "\n",
        "Risk assessment\n",
        "\n",
        "Consistency in loan approvals\n",
        "\n",
        "This leads to better financial decisions, reduced default risk, and fairer credit evaluation."
      ],
      "metadata": {
        "id": "mP6pSmR3zSWd"
      }
    }
  ]
}